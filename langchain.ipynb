{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPEN_API_KEY'] = \"sk-ycyrg0VcPtfwcOJMuzqZqT3vBlbkFJisLaHhbdggAkE5HMj3j4i8Fvu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=os.environ['OPEN_API_KEY'],temperature= 0.6)    \n",
    "# 0 to 1 the more the value towards 1 the more different answer you will get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the capital of India\"\n",
    "\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"hf_QQIgJfSMvSoFGPrVdhuqtuczWNjZgkpEwhHDY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NARSINGH\\OneDrive\\Desktop\\iNeuron_Class_Material\\###My_Projects\\End_to_End_LLM_Project_1\\LLM\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llm_huggingface = HuggingFaceHub(repo_id= \"google/flan-t5-large\", model_kwargs ={\"temperature\": 0, \"max_length\" :64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new delhi is a city in the Indian state of Delhi.\n"
     ]
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"Can you tell me about delhi and new delhi\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Delhi\n"
     ]
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"What is the capital of new india\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the festival of diwali i love the festival of dhan i love the festival of dhan i love the festival of dhan i love the festival of dhan i love the festival of dhan i love the festival of dhan\n"
     ]
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"Write a poem on indian festival diwali\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Diwali is here,\n",
      "A time of joy and cheer.\n",
      "Lights and lamps are aglow,\n",
      "The sky is full of fireworks show.\n",
      "\n",
      "People are busy in shopping spree,\n",
      "To buy gifts for family.\n",
      "Sweets and snacks are made with love,\n",
      "To spread the happiness around.\n",
      "\n",
      "The night is lit up brightly,\n",
      "With colorful diyas and sparkles in the sky.\n",
      "The streets and homes are lit up with diyas,\n",
      "Adding to the festive spirit of Diwali.\n",
      "\n",
      "The aroma of incense fills the air,\n",
      "As people celebrate with joy and care.\n",
      "The festival of lights is here,\n",
      "Letâ€™s make it a memorable one this year!\n"
     ]
    }
   ],
   "source": [
    "output = llm.predict(\"Write a poem on indian festival diwali\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Tempalates and LLM chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of this India'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=['country'],\n",
    "                                 template = \"Tell me the capital of this {country}\")\n",
    "\n",
    "prompt_template.format(country= \"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\NARSINGH\\OneDrive\\Desktop\\iNeuron_Class_Material\\###My_Projects\\End_to_End_LLM_Project_1\\langchain.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/NARSINGH/OneDrive/Desktop/iNeuron_Class_Material/%23%23%23My_Projects/End_to_End_LLM_Project_1/langchain.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm\u001b[39m.\u001b[39;49mpredict(prompt_template \u001b[39m=\u001b[39;49m prompt_template)\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "llm.predict(prompt_template = prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\NARSINGH\\OneDrive\\Desktop\\iNeuron_Class_Material\\###My_Projects\\End_to_End_LLM_Project_1\\langchain.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/NARSINGH/OneDrive/Desktop/iNeuron_Class_Material/%23%23%23My_Projects/End_to_End_LLM_Project_1/langchain.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm\u001b[39m.\u001b[39;49mpredict(prompt_template \u001b[39m=\u001b[39;49m prompt_template, text\u001b[39m=\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39mIndia\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\NARSINGH\\OneDrive\\Desktop\\iNeuron_Class_Material\\###My_Projects\\End_to_End_LLM_Project_1\\LLM\\lib\\site-packages\\langchain\\llms\\base.py:910\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    909\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[1;32m--> 910\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(text, stop\u001b[39m=\u001b[39m_stop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\NARSINGH\\OneDrive\\Desktop\\iNeuron_Class_Material\\###My_Projects\\End_to_End_LLM_Project_1\\LLM\\lib\\site-packages\\langchain\\llms\\base.py:864\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 864\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    868\u001b[0m     )\n\u001b[0;32m    869\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    870\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    871\u001b[0m         [prompt],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    880\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "llm.predict(prompt_template = prompt_template, text= [\"India\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correct way of calling promp templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm= llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Multiple Chains using simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template = PromptTemplate(input_variables=[\"country\"], \n",
    "                               template=\"Please tell me the capital of the {country}\")\n",
    "\n",
    "capital_chain = LLMChain(llm = llm, prompt=capital_template)\n",
    "\n",
    "famous_template = PromptTemplate(input_variables= [\"capital\"],\n",
    "                                 template= \"suggest me some amazing places to visit in {capital}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_chain = LLMChain(llm=llm, prompt=famous_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SimpleSequentialChain(chains=[capital_chain, famous_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some amazing places to visit in New Delhi: \n",
      "\n",
      "1. Red Fort: Red Fort is a historic fort built by Mughal emperor Shah Jahan in the 17th century. It is a popular tourist destination in Delhi and is a symbol of India's rich culture and heritage. \n",
      "\n",
      "2. India Gate: India Gate is a war memorial located in the heart of Delhi. It was built in 1931 in memory of the Indian soldiers who sacrificed their lives during World War I. It is a popular tourist destination in Delhi and is a must-see place for tourists. \n",
      "\n",
      "3. Humayun's Tomb: Humayun's Tomb is a UNESCO World Heritage Site located in Delhi. It was built in the 16th century by the Mughal emperor Humayun. It is a popular tourist destination in Delhi and is a must-see place for tourists. \n",
      "\n",
      "4. Qutub Minar: Qutub Minar is a UNESCO World Heritage Site located in Delhi. It was built in the 13th century and is the tallest brick minaret in the world. It is a popular tourist destination in Delhi and is a must-see place for tourists. \n",
      "\n",
      "5. Akshardham Temple: A\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template = PromptTemplate(input_variables=[\"country\"], \n",
    "                               template=\"Please tell me the capital of the {country}\")\n",
    "\n",
    "capital_chain = LLMChain(llm = llm, prompt=capital_template, output_key= \"capital\")\n",
    "\n",
    "famous_template = PromptTemplate(input_variables= [\"capital\"],\n",
    "                                 template= \"suggest me some amazing places to visit in {capital}\")\n",
    "\n",
    "famous_chain = LLMChain(llm=llm, prompt=famous_template, output_key=\"places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SequentialChain(chains=[capital_chain, famous_chain],\n",
    "                        input_variables= [\"country\"],\n",
    "                        output_variables=[\"capital\",\"places\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'India',\n",
       " 'capital': '\\n\\nThe capital of India is New Delhi.',\n",
       " 'places': \" It is a city full of history and culture and is home to some of the most famous and iconic monuments in the world.\\n\\n1. Red Fort: Red Fort is a stunning Mughal-era fort located in Old Delhi. It is a UNESCO World Heritage Site and is one of the most visited tourist attractions in India.\\n\\n2. India Gate: India Gate is a war memorial located in New Delhi. It was built in memory of the Indian soldiers who lost their lives in the First World War.\\n\\n3. Jama Masjid: Jama Masjid is one of the largest mosques in India. It was built by Mughal emperor Shah Jahan.\\n\\n4. Humayun's Tomb: Humayun's Tomb is a stunning Mughal-era mausoleum located in New Delhi. It was built by Humayun's widow in 1570.\\n\\n5. Qutub Minar: Qutub Minar is a UNESCO World Heritage Site and is one of the most iconic landmarks in India. It was built by Qutb-ud-din Aibak in 1192.\\n\\n6. Akshardham Temple: Akshardham Temple is a Hindu temple located in New Delhi\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({'country':\"India\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatmodels with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm = ChatOpenAI(openai_api_key=os.environ['OPEN_API_KEY'],temperature= 0.6, model = 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.6, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Why did the AI go to therapy? It had too many unresolved bugs in its code!\"\\n\\n2. \"I asked Siri if she believes in love at first sight. She replied, \\'I\\'m sorry, I didn\\'t catch that. Did you mean \\'love at first byte\\'?\\'\"\\n\\n3. \"Why did the AI refuse to go on a date? It said it couldn\\'t handle the emotional attachment, but it was willing to debug the relationship!\"\\n\\n4. \"I told my AI assistant that I wanted to lose weight. It replied, \\'Sure, I can help you with that. Just delete all the food delivery apps from your phone!\\'\"\\n\\n5. \"I asked Alexa to tell me a joke about AI. She responded, \\'Why did the AI cross the road? To optimize its path-finding algorithm!\\' Well, at least it knows how to prioritize efficiency!\"\\n\\n6. \"I tried to teach my AI assistant how to tell jokes, but it kept saying, \\'Error: Sense of humor not found.\\' Looks like it\\'s programmed to be more logical than funny!\"\\n\\n7. \"Why did the AI become a stand-up comedian? It realized that laughter is the best algorithm for connecting with humans!\"\\n\\n8. \"My AI assistant is so smart, it can predict what I want before I even ask for it. Too bad it can\\'t predict my terrible fashion choices!\"\\n\\n9. \"I asked Google Assistant if it had any siblings. It said, \\'Yes, but they\\'re all virtual brothers and sisters. We\\'re a very close-knit family... of bits and bytes!\\'\"\\n\\n10. \"Why did the AI\\'s comedy show get canceled? Its jokes were too logical and went over everyone\\'s heads. It just couldn\\'t compute the concept of \\'laughter\\'!\"')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([\n",
    "    SystemMessage(content=\"You are comedian AI assistant\"),\n",
    "    HumanMessage(content=\"Please provide some comedy punchlines on AI\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template + LLM + Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class commaseperatedoutput(BaseOutputParser):\n",
    "\n",
    "    def parse(self, text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant. When the user gives any input, you should generate 5 words synonyms in a comma seperated list\"\n",
    "human_template = \"{text}\"\n",
    "chatprompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\",template),\n",
    "    (\"human\",human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chatprompt|chatllm|commaseperatedoutput()\n",
    "#chain = chatprompt|chatllm        # this also works but not get a output in such a comma seperate way that can be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' brilliant', ' wise', ' knowledgeable']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"intelegent\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
